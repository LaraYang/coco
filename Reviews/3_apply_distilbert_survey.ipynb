{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.9.1\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sys\n",
    "import os\n",
    "import re\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "import string\n",
    "import contractions\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "from textblob import TextBlob\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "# Copy the line below to chunks where we want to set the column width to max\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "import spacy\n",
    "import torch\n",
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing Survey Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "survey_hr_df = pd.read_csv('/Users/Lara/Documents/CompCulture/spacespace/Coco/analyses_data/preprocessed_survey_hr.csv')\n",
    "survey_text_df = survey_hr_df.dropna(subset=['pros', 'cons', 'story']).astype({'pros':'str',\n",
    "                             'cons':'str',\n",
    "                             'story':'str'})\n",
    "survey_text_df = survey_text_df.reset_index()\n",
    "survey_text_df['race'] = survey_text_df['Race'].apply(lambda x : 'Other' if x in ['Black or African American', 'Missing', 'Native Hawaiian or Other Pacific Islander'] else x)\n",
    "survey_text_df = survey_text_df.drop(columns=['ResponseId', 'LocationLatitude', 'LocationLongitude', \"LINK\", \"Race\"]\n",
    "                    + [\"mael_\"+str(i) for i in range(1, 7)])\n",
    "                                \n",
    "tweet_tokenizer = TweetTokenizer()\n",
    "punctuation = string.punctuation + '–...…’“”'\n",
    "re_number = r\"[0-9]+(\\.)?\"\n",
    "# identify whether there are three or more repeats of the same characters; used to get rid of filler words\n",
    "re_repeat_digits = r'([a-z])\\1{2}'\n",
    "# necessary to make sure we don't confuse US the country with us the first person pronoun\n",
    "us_str = 'u_s_a'\n",
    "for col_name in ['pros', 'cons', 'story']:\n",
    "    survey_text_df[col_name+'_cleaned'] = survey_text_df[col_name].apply(\n",
    "        lambda x: x.replace('USA', us_str).replace('US', us_str).replace(\n",
    "            'U.S.A.', us_str).replace('U.S.A', us_str).replace('U.S.', us_str).replace('U.S', us_str)\n",
    "    ).apply(contractions.fix).str.lower().apply(lambda x: x.replace('n/a', ''))\n",
    "    \n",
    "    survey_text_df[col_name+'_toks'] = survey_text_df[col_name+'_cleaned'].apply(\n",
    "        lambda x : x.replace('.', ' ')).apply(tweet_tokenizer.tokenize).apply(\n",
    "        lambda toks : [t for t in toks if t not in punctuation\n",
    "                       and re.match(re_number, t) is None and re.search(re_repeat_digits, t) is None])\n",
    "    \n",
    "    survey_text_df[col_name+'_toks_len'] = survey_text_df[col_name+'_toks'].apply(len)\n",
    "\n",
    "survey_text_df['glassdoor_cleaned'] = survey_text_df.apply(lambda row: row['pros_cleaned'] + ' ' + row['cons_cleaned'], axis=1)\n",
    "\n",
    "survey_text_df['low_quality_response'] = np.where((survey_text_df['pros'] == survey_text_df['cons']) |\n",
    "                                                  (survey_text_df['cons'] == survey_text_df['story']) | \n",
    "                                                  (survey_text_df['pros'] == survey_text_df['story']),\n",
    "                                                  1, 0)\n",
    "# changed threshold from 3 in exploring_survey_responses.ipynb to 5 as we are not removing stop words here\n",
    "survey_text_df['low_quality_pros'] = survey_text_df.apply(lambda row :\n",
    "                                                          1 if (row['pros_toks_len'] < 5 | (re.search(re_repeat_digits, row['pros']) is not None))\n",
    "                                                          else row['low_quality_response'], axis=1)\n",
    "survey_text_df['low_quality_cons'] = survey_text_df.apply(lambda row :\n",
    "                                                          1 if (row['cons_toks_len'] < 5 | (re.search(re_repeat_digits, row['cons']) is not None))\n",
    "                                                          else row['low_quality_response'], axis=1)\n",
    "survey_text_df['low_quality_story'] = survey_text_df.apply(lambda row :\n",
    "                                                           1 if (row['story_toks_len'] < 5 | (re.search(re_repeat_digits, row['story']) is not None))\n",
    "                                                           else row['low_quality_response'], axis=1)                                     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing Glassdoor Data for Finetuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-4073475a1d56>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mglassdoor_data_dir\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"/Users/Lara/Documents/Stanford/Research/Glassdoor/\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mglassdoor_reviews\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mglassdoor_data_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"reviews_new_processed.csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mglassdoor_reviews\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mglassdoor_reviews\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mglassdoor_reviews\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'is_current_job'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mglassdoor_reviews\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mglassdoor_reviews\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropna\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msubset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'pros'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'cons'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'pros'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m'str'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'cons'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m'str'\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    684\u001b[0m     )\n\u001b[1;32m    685\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 686\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    687\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    688\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    456\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    457\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 458\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    459\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    460\u001b[0m         \u001b[0mparser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m   1184\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnrows\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1185\u001b[0m         \u001b[0mnrows\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_validate_integer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"nrows\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1186\u001b[0;31m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1187\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1188\u001b[0m         \u001b[0;31m# May alter columns / col_dict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m   2143\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnrows\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2144\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2145\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2146\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2147\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_first_chunk\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.read\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._read_low_memory\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._read_rows\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._convert_column_data\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._convert_tokens\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._convert_with_dtype\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/core/dtypes/common.py\u001b[0m in \u001b[0;36mis_categorical_dtype\u001b[0;34m(arr_or_dtype)\u001b[0m\n\u001b[1;32m    528\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    529\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 530\u001b[0;31m \u001b[0;32mdef\u001b[0m \u001b[0mis_categorical_dtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marr_or_dtype\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    531\u001b[0m     \"\"\"\n\u001b[1;32m    532\u001b[0m     \u001b[0mCheck\u001b[0m \u001b[0mwhether\u001b[0m \u001b[0man\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mlike\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mof\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mCategorical\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "glassdoor_data_dir = \"/Users/Lara/Documents/Stanford/Research/Glassdoor/\"\n",
    "glassdoor_reviews = pd.read_csv(os.path.join(glassdoor_data_dir, \"reviews_new_processed.csv\"))\n",
    "glassdoor_reviews = glassdoor_reviews.loc[glassdoor_reviews['is_current_job'] == 1,]\n",
    "\n",
    "glassdoor_reviews = glassdoor_reviews.dropna(subset=['pros', 'cons']).astype({'pros':'str', 'cons':'str'})\n",
    "\n",
    "glassdoor_reviews['pros_cleaned'] = glassdoor_reviews['pros'].apply(\n",
    "    lambda x: x.replace('USA', us_str).replace('US', us_str).replace(\n",
    "        'U.S.A.', us_str).replace('U.S.A', us_str).replace('U.S.', us_str).replace('U.S', us_str)\n",
    ").apply(contractions.fix).str.lower().apply(lambda x: x.replace('n/a', ''))\n",
    "\n",
    "glassdoor_reviews['cons_cleaned'] = glassdoor_reviews['cons'].apply(\n",
    "    lambda x: x.replace('USA', us_str).replace('US', us_str).replace(\n",
    "        'U.S.A.', us_str).replace('U.S.A', us_str).replace('U.S.', us_str).replace('U.S', us_str)\n",
    ").apply(contractions.fix).str.lower().apply(lambda x: x.replace('n/a', ''))\n",
    "\n",
    "\n",
    "glassdoor_reviews['glassdoor_cleaned'] = glassdoor_reviews.apply(lambda row: row['pros_cleaned'] + ' ' + row['cons_cleaned'], axis=1)\n",
    "filepath = '/Users/Lara/Documents/CompCulture/spacespace/Coco/helper_data/glassdoor_distilbert_corpus.txt'\n",
    "np.savetxt(filepath, glassdoor_reviews['glassdoor_cleaned'], fmt=\"%s\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finetuning Distilbert on Glassdoor Data \n",
    "This portion of the code is run Google Colab given slow training speed on CPU"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
    "import torch.utils.data as data_utils\n",
    "from transformers import LineByLineTextDataset, Trainer, TrainingArguments, DataCollatorForLanguageModeling\n",
    "import csv\n",
    "import numpy as np\n",
    "from transformers import DistilBertTokenizer, DistilBertForMaskedLM, BertForMaskedLM, BertTokenizerFast\n",
    "import torch\n",
    "\n",
    "filepath = '/Users/Lara/Documents/CompCulture/spacespace/Coco/helper_data/glassdoor_sampled_corpus.txt'\n",
    "tokenizer = BertTokenizerFast.from_pretrained('bert-base-uncased')\n",
    "model = DistilBertForMaskedLM.from_pretrained('distilbert-base-uncased')\n",
    "\n",
    "dataset = LineByLineTextDataset(\n",
    "    tokenizer=tokenizer,\n",
    "    file_path=filepath,\n",
    "    block_size=128,\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',          # output directory\n",
    "    num_train_epochs=1,              # total # of training epochs\n",
    "    per_device_train_batch_size=16,  # batch size per device during training\n",
    "    per_device_eval_batch_size=64,   # batch size for evaluation\n",
    "    warmup_steps=500,                # number of warmup steps for learning rate scheduler\n",
    "    weight_decay=0.01,               # strength of weight decay\n",
    "    logging_dir='./logs',            # directory for storing logs\n",
    ")\n",
    "\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer, mlm=True, mlm_probability=0.15\n",
    ")\n",
    "trainer = Trainer(\n",
    "    model=model,                         # the instantiated 🤗 Transformers model to be trained\n",
    "    args=training_args,                  # training arguments, defined above\n",
    "    train_dataset=dataset,         # training dataset\n",
    "    data_collator=data_collator,\n",
    ")\n",
    "%%time\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "trainer.save_model(\"./finetuned_distilbert\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculating \"We-ness\" of Language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Original Measure\n",
    "type2pronouns = {'subj': ['i', 'you', 'he', 'she', 'we', 'they'],\n",
    "           'obj': ['me', 'you', 'him', 'her', 'us', 'them'],\n",
    "           'poss': ['my', 'your', 'his', 'her', 'our', 'their'],\n",
    "           'attr': ['mine', 'yours', 'his', 'hers', 'ours', 'theirs'],\n",
    "           'npadvmod': ['myself', 'yourself', 'yourselves', 'himself', 'herself', 'ourselves', 'themselves']}\n",
    "\n",
    "def average_with_nan(elem1, elem2):\n",
    "    if np.isnan(elem1) and np.isnan(elem2):\n",
    "        return np.nan\n",
    "    elif np.isnan(elem1):\n",
    "        return elem2\n",
    "    elif np.isnan(elem2):\n",
    "        return elem1\n",
    "    else:\n",
    "        return (elem1+elem2)/2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "def get_token_type(text):\n",
    "    doc = nlp(text)\n",
    "    index2type = {}\n",
    "    for token in doc:\n",
    "        index2type[(token.idx, token.idx+len(token.text))] = token.dep_\n",
    "    return index2type\n",
    "\n",
    "def we_prob(text, tokenizer, model, type2pronouns, filter_we=True, we_index=-2):\n",
    "    \"\"\"\n",
    "    Returns a tuple containing the probability of we-words and probability of we-words\n",
    "    as weighted by the total probability of relevant pronouns, where relevant pronouns\n",
    "    refer to pronouns of the same type, i.e., subject to subject, possessive to possessive.\n",
    "    The reason that type matching is important is due to the model's ability to \n",
    "    infer correct type based on context, which needs to be accounted for when weighting probabilities.\n",
    "    \"\"\"\n",
    "    text = text.lower()\n",
    "    encoded_dict = tokenizer(text, return_offsets_mapping=True)\n",
    "    toks = tokenizer.convert_ids_to_tokens(encoded_dict['input_ids'])\n",
    "    pronoun2type = {pronoun : k for k, v in type2pronouns.items() for pronoun in v}\n",
    "    pronouns = [pronoun for k, v in type2pronouns.items() for pronoun in v]\n",
    "\n",
    "    found_we_word = True if len(set(toks) & set(we_words)) > 0 else False\n",
    "    found_pronouns = True if len(set(toks) & set(pronouns)) > 0 else False\n",
    "    if (filter_we and not found_we_word) or not found_pronouns:\n",
    "        return np.nan\n",
    "    index2type = get_token_type(text)\n",
    "    # with pytorch format, the results can't be converted to tokens directly\n",
    "    # if direct conversation needed, remove return_tensors argument\n",
    "    input_ids = tokenizer(text, return_tensors=\"pt\")[\"input_ids\"]\n",
    "    # chopping input_ids to deal with cases where there are more than 512 tokens\n",
    "    # there is currently only one instance with cons where more than 512 tokens exist\n",
    "    # if this method is actually used to build model for glassdoor, should explore\n",
    "    # other ways to deal with this problem - e.g., sliding window\n",
    "    if input_ids.shape[1] > 512: \n",
    "        input_ids = input_ids[:, 0:511]\n",
    "        toks = toks[0:511]\n",
    "    masked_index = []\n",
    "    skip = False\n",
    "    # mask all pronouns to prevent leaking.\n",
    "    for i in range(input_ids.shape[1]):\n",
    "        if skip:\n",
    "            skip = False\n",
    "            continue\n",
    "        curr_id = input_ids[0, i]\n",
    "        curr_tok = tokenizer.convert_ids_to_tokens(curr_id.item())\n",
    "        if curr_tok in pronouns:\n",
    "            input_ids[0, i] = tokenizer.mask_token_id\n",
    "            # the following word has to be a verb, which needs to be\n",
    "            # masked due to verb tenses leaking the right pronoun\n",
    "            if pronoun2type[curr_tok] == 'nsubj':\n",
    "                input_ids[0, i+1] = tokenizer.mask_token_id\n",
    "                skip = True             \n",
    "            if not filter_we:\n",
    "                masked_index.append(i)\n",
    "            elif curr_tok in we_words:\n",
    "                masked_index.append(i)\n",
    "    outputs = model(input_ids)\n",
    "    prediction_logits = t.logits\n",
    "    raw_prob, prop_prob = [], []\n",
    "    for m in masked_index:\n",
    "        orig_word = toks[m]           \n",
    "        target_words = type2pronouns[pronoun2type[orig_word]]\n",
    "        # \"you\" could be a subject or an object\n",
    "        if orig_word == 'you':\n",
    "            if encoded_dict['offset_mapping'][m] not in index2type.keys():\n",
    "                raise ValueError('Token has no matching type. See text %s:' % text)\n",
    "            else:\n",
    "                # obj, pobj, dobj, dative - indirect object\n",
    "                if 'obj' in index2type[encoded_dict['offset_mapping'][m]] or 'dative' == index2type[encoded_dict['offset_mapping'][m]]:\n",
    "                    target_words = type2pronouns['obj']\n",
    "                elif 'subj' in index2type[encoded_dict['offset_mapping'][m]]:\n",
    "                    target_words = type2pronouns['subj']\n",
    "                else:\n",
    "                    raise ValueError('Unexpected dependency type: %s' % index2type[encoded_dict['offset_mapping'][m]])\n",
    "        we_word = target_words[we_index]\n",
    "        logits = prediction_logits[0, m, :]\n",
    "        # the original code for single mask use case uses dim=0 as there is only one dim\n",
    "        # which also works here given that we are iterating across masks one at a time\n",
    "        probs = logits.softmax(dim=-1)\n",
    "        target_inds = np.array(tokenizer.convert_tokens_to_ids(target_words))\n",
    "        # of shape number of we-word masks by number of target words\n",
    "        values = probs[..., target_inds]\n",
    "        we_word_prob = probs[..., tokenizer.convert_tokens_to_ids(we_word)].item()\n",
    "        we_word_prop = we_word_prob / values.sum().item()\n",
    "        raw_prob.append(we_word_prob)\n",
    "        prop_prob.append(we_word_prop)\n",
    "    return (sum(raw_prob)/len(raw_prob), sum(prop_prob)/len(prop_prob))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DistilBertForMaskedLM, BertTokenizerFast\n",
    "tokenizer = BertTokenizerFast.from_pretrained('bert-base-uncased')\n",
    "i_words = ['i', 'me', 'my', 'mine', 'myself']\n",
    "we_words = ['we', 'us', 'our', 'ours', 'ourselves']\n",
    "they_words = ['they', 'them', 'their', 'theirs', 'themselves']\n",
    "\n",
    "model = DistilBertForMaskedLM.from_pretrained(\"./glassdoor_finetuned_distilbert\", return_dict=True)\n",
    "# Uses All Pronouns (except for it - too many typos)\n",
    "we_prob_pros = survey_text_df['pros_cleaned'].apply(lambda text : we_prob(text, tokenizer, model, type2pronouns, filter_we=False))\n",
    "we_prob_cons = survey_text_df['cons_cleaned'].apply(lambda text : we_prob(text, tokenizer, model, type2pronouns, filter_we=False))\n",
    "we_prob_pros_df = pd.DataFrame(we_prob_pros.tolist(), columns=['pros_finetuned_prob', 'pros_finetuned_prop'])\n",
    "we_prob_cons_df = pd.DataFrame(we_prob_cons.tolist(), columns=['cons_finetuned_prob', 'cons_finetuned_prop'])\n",
    "survey_text_df = pd.concat([survey_text_df, we_prob_pros_df, we_prob_cons_df], axis=1)\n",
    "\n",
    "survey_text_df['glassdoor_finetuned_prob'] = survey_text_df.apply(lambda row : average_with_nan(row['pros_finetuned_prob'], row['cons_finetuned_prob']), axis=1)\n",
    "survey_text_df['glassdoor_finetuned_prop'] = survey_text_df.apply(lambda row : average_with_nan(row['pros_finetuned_prop'], row['cons_finetuned_prop']), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "survey_text_df = survey_text_df.drop('index', axis=1)\n",
    "survey_text_df.to_csv('~/Documents/CompCulture/spacespace/Coco/analyses_data/survey_hr_glassdoor_distilbert.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
