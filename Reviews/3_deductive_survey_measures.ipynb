{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-15T02:44:40.673269Z",
     "start_time": "2021-12-15T02:44:33.057256Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sys\n",
    "import os\n",
    "import re\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "import string\n",
    "import contractions\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "from textblob import TextBlob\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "#from Process.liwc_index import collapse_by_liwc, liwc_term_list_to_dict, liwc_cat_map\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "from transformers import pipeline, AutoTokenizer, AutoModelForSequenceClassification\n",
    "# default model for sentiment analysis is distilbert\n",
    "pipe = pipeline('sentiment-analysis')\n",
    "from transformers import DistilBertTokenizer, DistilBertForMaskedLM, BertForMaskedLM\n",
    "import torch\n",
    "# Copy the line below to chunks where we want to set the column width to max\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "from transformers import BertTokenizerFast\n",
    "import spacy"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Deductive Steps\n",
    "\n",
    "trust/family vs achievement oriented language\n",
    "On high end:\n",
    "- Trust\n",
    "- Growth/Climbing/Opportunities\n",
    "- Family\n",
    "- Attachment\n",
    "- Care\n",
    "- Community\n",
    "- Identity\n",
    "- Pride\n",
    "- Appreciation\n",
    "- Connected\n",
    "- Best\n",
    "Low end:\n",
    "- Achievement\n",
    "- Career sometimes\n",
    "- profit driven language\n",
    "- Self oriented language\n",
    "Other ideas:\n",
    "Degree of sentiment important\n",
    "Sentiment * energy in language: high+ high = high idtf, low + low = low idtf\n",
    "High identification uses a lot of energetic language\n",
    "\"they\" can be resolved through co-reference resolution. If they refers to leadership, low, if they refers to clients, high\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "survey_hr_df = pd.read_csv('~/Documents/CompCulture/spacespace/Coco/analyses_data/preprocessed_survey_hr.csv')\n",
    "survey_text_df = survey_hr_df.dropna(subset=['pros', 'cons', 'story']).astype({'pros':'str',\n",
    "                             'cons':'str',\n",
    "                             'story':'str'})\n",
    "survey_text_df = survey_text_df.reset_index()\n",
    "survey_text_df['disengagement_3'] = 4 - survey_text_df['disengagement_3']\n",
    "survey_text_df['exhaustion_2'] = 4 - survey_text_df['exhaustion_2']\n",
    "survey_text_df['disengagement'] = survey_text_df[['disengagement_1', 'disengagement_2', 'disengagement_3']].mean(axis=1)\n",
    "survey_text_df['exhaustion'] = survey_text_df[['exhaustion_1', 'exhaustion_2', 'exhaustion_3']].mean(axis=1)\n",
    "survey_text_df['burnout'] = survey_text_df[['disengagement', 'exhaustion']].mean(axis=1)\n",
    "survey_text_df['race'] = survey_text_df['Race'].apply(lambda x : 'Other' if x in ['Black or African American', 'Missing', 'Native Hawaiian or Other Pacific Islander'] else x)\n",
    "survey_text_df = survey_text_df.drop(columns=['ResponseId', 'LocationLatitude', 'LocationLongitude', \"LINK\", \"Race\"]\n",
    "                    + [\"mael_\"+str(i) for i in range(1, 7)]\n",
    "                    + [\"disengagement_\"+str(i) for i in range(1, 4)]\n",
    "                    + [\"exhaustion_\"+str(i) for i in range(1, 4)])"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": null,
=======
   "execution_count": 4,
>>>>>>> a663c3c746110d11758807c2da282d5669fd297a
   "metadata": {},
   "outputs": [],
   "source": [
    "i_words = ['i', 'me', 'my', 'mine', 'myself']\n",
    "we_words = ['we', 'us', 'our', 'ours', 'ourselves']\n",
    "they_words = ['they', 'them', 'their', 'theirs', 'themselves']\n",
    "def count_we_i(df):\n",
    "    we = sum([1 for t in df['pros_toks'] if t in we_words])\n",
    "    we += sum([1 for t in df['cons_toks'] if t in we_words])\n",
    "    i = sum([1 for t in df['pros_toks'] if t in i_words])\n",
    "    i += sum([1 for t in df['cons_toks'] if t in i_words])\n",
    "    log_we_i = np.log(we/i) if i > 0 and we > 0 else np.nan\n",
    "    return log_we_i\n",
    "\n",
    "def count_we_they(df):\n",
    "    we = sum([1 for t in df['pros_toks'] if t in we_words])\n",
    "    we += sum([1 for t in df['cons_toks'] if t in we_words])\n",
    "    they = sum([1 for t in df['pros_toks'] if t in they_words])\n",
    "    they += sum([1 for t in df['cons_toks'] if t in they_words])\n",
    "    log_we_they = np.log(we/they) if we > 0 and they > 0 else np.nan\n",
    "    return log_we_they\n",
    "\n",
    "def count(df, words, colname):\n",
    "    return sum([1 for t in df[colname] if t in words])"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": null,
=======
   "execution_count": 5,
>>>>>>> a663c3c746110d11758807c2da282d5669fd297a
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_tokenizer = TweetTokenizer()\n",
    "punctuation = string.punctuation + '‚Äì...‚Ä¶‚Äô‚Äú‚Äù'\n",
    "re_number = r\"[0-9]+(\\.)?\"\n",
    "# identify whether there are three or more repeats of the same characters; used to get rid of filler words\n",
    "re_repeat_digits = r'([a-z])\\1{2}'\n",
    "# necessary to make sure we don't confuse US the country with us the first person pronoun\n",
    "us_str = 'u_s_a'\n",
    "for col_name in ['pros', 'cons', 'story']:\n",
    "    survey_text_df[col_name+'_cleaned'] = survey_text_df[col_name].apply(\n",
    "        lambda x: x.replace('USA', us_str).replace('US', us_str).replace(\n",
    "            'U.S.A.', us_str).replace('U.S.A', us_str).replace('U.S.', us_str).replace('U.S', us_str)\n",
    "    ).apply(contractions.fix).str.lower().apply(lambda x: x.replace('n/a', ''))\n",
    "    \n",
    "    survey_text_df[col_name+'_toks'] = survey_text_df[col_name+'_cleaned'].apply(\n",
    "        lambda x : x.replace('.', ' ')).apply(tweet_tokenizer.tokenize).apply(\n",
    "        lambda toks : [t for t in toks if t not in punctuation\n",
    "                       and re.match(re_number, t) is None and re.search(re_repeat_digits, t) is None])\n",
    "    \n",
    "    survey_text_df[col_name+'_toks_len'] = survey_text_df[col_name+'_toks'].apply(len)\n",
    "\n",
    "# ordering different from 2_preprocess_survey_modeling as we are not including any pronouns in our list of \n",
    "# stop words, allowing all apply functions to be written consecutively\n",
    "survey_text_df['pros_we'] = survey_text_df.apply(count, args=(we_words, 'pros_toks',), axis=1)\n",
    "survey_text_df['pros_i'] = survey_text_df.apply(count, args=(i_words, 'pros_toks'), axis=1)\n",
    "survey_text_df['pros_they'] = survey_text_df.apply(count, args=(they_words, 'pros_toks'), axis=1)\n",
    "survey_text_df['cons_we'] = survey_text_df.apply(count, args=(we_words, 'cons_toks'), axis=1)\n",
    "survey_text_df['cons_i'] = survey_text_df.apply(count, args=(i_words, 'cons_toks'), axis=1)\n",
    "survey_text_df['cons_they'] = survey_text_df.apply(count, args=(they_words, 'cons_toks'), axis=1)\n",
    "survey_text_df['story_we'] = survey_text_df.apply(count, args=(we_words, 'story_toks'), axis=1)\n",
    "survey_text_df['story_i'] = survey_text_df.apply(count, args=(i_words, 'story_toks'), axis=1)\n",
    "survey_text_df['story_they'] = survey_text_df.apply(count, args=(they_words, 'story_toks'), axis=1)\n",
    "\n",
    "survey_text_df['glassdoor_toks'] = survey_text_df.apply(lambda row : row['pros_toks'] + row['cons_toks'], axis=1)\n",
    "survey_text_df['glassdoor_cleaned'] = survey_text_df.apply(lambda row: row['pros_cleaned'] + ' ' + row['cons_cleaned'], axis=1)\n",
    "survey_text_df['glassdoor_toks_len'] = survey_text_df['glassdoor_toks'].apply(len)\n",
    "\n",
    "survey_text_df['low_quality_response'] = np.where((survey_text_df['pros'] == survey_text_df['cons']) |\n",
    "                                                  (survey_text_df['cons'] == survey_text_df['story']) | \n",
    "                                                  (survey_text_df['pros'] == survey_text_df['story']),\n",
    "                                                  1, 0)\n",
    "# changed threshold from 3 in exploring_survey_responses.ipynb to 5 as we are not removing stop words here\n",
    "survey_text_df['low_quality_pros'] = survey_text_df.apply(lambda row :\n",
    "                                                          1 if (row['pros_toks_len'] < 5 | (re.search(re_repeat_digits, row['pros']) is not None))\n",
    "                                                          else row['low_quality_response'], axis=1)\n",
    "survey_text_df['low_quality_cons'] = survey_text_df.apply(lambda row :\n",
    "                                                          1 if (row['cons_toks_len'] < 5 | (re.search(re_repeat_digits, row['cons']) is not None))\n",
    "                                                          else row['low_quality_response'], axis=1)\n",
    "survey_text_df['low_quality_story'] = survey_text_df.apply(lambda row :\n",
    "                                                           1 if (row['story_toks_len'] < 5 | (re.search(re_repeat_digits, row['story']) is not None))\n",
    "                                                           else row['low_quality_response'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LIWC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "survey_text_df['liwc_counts_pros'] = survey_text_df['pros_toks'].apply(lambda toks : liwc_term_list_to_dict(collapse_by_liwc(toks)))\n",
    "survey_text_df['liwc_counts_cons'] = survey_text_df['cons_toks'].apply(lambda toks : liwc_term_list_to_dict(collapse_by_liwc(toks)))\n",
    "survey_text_df['liwc_counts_total'] = survey_text_df.apply(lambda row :\n",
    "        {'liwc_' + key: row['liwc_counts_pros'].get(key, 0) + row['liwc_counts_cons'].get(key, 0) for key in liwc_cat_map}, axis=1)\n",
    "\n",
    "liwc_counts_pros = survey_text_df['liwc_counts_pros'].apply(pd.Series)\n",
    "liwc_counts_cons = survey_text_df['liwc_counts_cons'].apply(pd.Series)\n",
    "\n",
    "liwc_counts_pros.columns = ['pros_liwc_' + str(col) for col in liwc_counts_pros.columns]\n",
    "liwc_counts_cons.columns = ['cons_liwc_' + str(col) for col in liwc_counts_cons.columns]\n",
    "# Note: this cell should only be run once as repeat runs will append multiple liwc categories to the dataset\n",
    "survey_text_df = pd.concat([survey_text_df, liwc_counts_pros, liwc_counts_cons,\n",
    "                             survey_text_df['liwc_counts_total'].apply(pd.Series)], axis=1)\n",
    "survey_text_df = survey_text_df.drop(columns=['liwc_counts_pros', 'liwc_counts_cons', 'liwc_counts_total'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "survey_text_df['pros_we_pronoun_prop'] = survey_text_df.apply(lambda row : row['pros_we'] / row['liwc_Ppron'] if row['liwc_Ppron'] > 0 else np.nan, axis=1)\n",
    "survey_text_df['cons_we_pronoun_prop'] = survey_text_df.apply(lambda row : row['cons_we'] / row['liwc_Ppron'] if row['liwc_Ppron'] > 0 else np.nan, axis=1)\n",
    "survey_text_df['we_pronoun_prop'] = survey_text_df.apply(lambda row : (row['pros_we'] + row['cons_we']) / row['liwc_Ppron'] if row['liwc_Ppron'] > 0 else np.nan, axis=1)\n",
    "\n",
    "survey_text_df['pros_they_pronoun_prop'] = survey_text_df.apply(lambda row : row['pros_they'] / row['liwc_Ppron'] if row['liwc_Ppron'] > 0 else np.nan, axis=1)\n",
    "survey_text_df['cons_they_pronoun_prop'] = survey_text_df.apply(lambda row : row['cons_they'] / row['liwc_Ppron'] if row['liwc_Ppron'] > 0 else np.nan, axis=1)\n",
    "survey_text_df['they_pronoun_prop'] = survey_text_df.apply(lambda row : (row['pros_they'] + row['cons_they']) / row['liwc_Ppron'] if row['liwc_Ppron'] > 0 else np.nan, axis=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "survey_text_df['pros_tb_sentiment'] = survey_text_df['pros_cleaned'].apply(lambda sent : TextBlob(sent).sentiment[0])\n",
    "survey_text_df['pros_tb_subjectivity'] = survey_text_df['pros_cleaned'].apply(lambda sent : TextBlob(sent).sentiment[1])\n",
    "survey_text_df['cons_tb_sentiment'] = survey_text_df['cons_cleaned'].apply(lambda sent : TextBlob(sent).sentiment[0])\n",
    "survey_text_df['cons_tb_subjectivity'] = survey_text_df['cons_cleaned'].apply(lambda sent : TextBlob(sent).sentiment[1])\n",
    "survey_text_df['glassdoor_tb_sentiment'] = survey_text_df['glassdoor_cleaned'].apply(lambda sent : TextBlob(sent).sentiment[0])\n",
    "survey_text_df['glassdoor_tb_subjectivity'] = survey_text_df['glassdoor_cleaned'].apply(lambda sent : TextBlob(sent).sentiment[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (718 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (550 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (573 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (871 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (826 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    }
   ],
   "source": [
    "# The default model used in pipe is DistilBert, which uses the same tokenizer as the Bert tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"nlptown/bert-base-multilingual-uncased-sentiment\")\n",
    "survey_text_df['pros_bert_toks_len'] = survey_text_df['pros_cleaned'].apply(\n",
    "    lambda sent : len(tokenizer(sent)['input_ids']))\n",
    "survey_text_df['cons_bert_toks_len'] = survey_text_df['cons_cleaned'].apply(\n",
    "    lambda sent : len(tokenizer(sent)['input_ids']))\n",
    "survey_text_df['glassdoor_bert_toks_len'] = survey_text_df['glassdoor_cleaned'].apply(\n",
    "    lambda sent : len(tokenizer(sent)['input_ids']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# no rows in pros exceed 512 tokens and thus no filtering is done here\n",
    "survey_text_df['pros_db_sentiment'] = survey_text_df['pros_cleaned'].apply(lambda sent : pipe(sent)[0]['label'])\n",
    "survey_text_df['pros_db_score'] = survey_text_df['pros_cleaned'].apply(lambda sent : pipe(sent)[0]['score'])\n",
    "\n",
    "survey_text_df['cons_db_sentiment'] = survey_text_df.apply(lambda row :\n",
    "    pipe(row['cons_cleaned'])[0]['label'] if row['cons_bert_toks_len'] <= 512 else np.nan, axis=1)\n",
    "survey_text_df['cons_db_score'] = survey_text_df.apply(lambda row :\n",
    "    pipe(row['cons_cleaned'])[0]['score'] if row['cons_bert_toks_len'] <= 512 else np.nan, axis=1)\n",
    "survey_text_df['glassdoor_db_sentiment'] = survey_text_df.apply(lambda row :\n",
    "    pipe(row['glassdoor_cleaned'])[0]['label'] if row['glassdoor_bert_toks_len'] <= 512 else np.nan, axis=1)\n",
    "survey_text_df['glassdoor_db_score'] = survey_text_df.apply(lambda row :\n",
    "    pipe(row['glassdoor_cleaned'])[0]['score'] if row['glassdoor_bert_toks_len'] <= 512 else np.nan, axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "survey_text_df = survey_text_df.drop(columns=['pros_bert_toks_len', 'cons_bert_toks_len', 'glassdoor_bert_toks_len'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Energy or Arousal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper functions for calculating arousal\n",
    "def list_mean(l, length=None):\n",
    "    \"\"\"\n",
    "    Calculates the mean of a list of float valence or arousal values associated with words.\n",
    "    If no length is provided, returns mean of list.\n",
    "    If length is provided, returns mean of list where we assume words not contained in lexicon\n",
    "    have valence or arousal vlaues of 0.5. The length argument signals how many words were in original list\n",
    "    for which valence or arousal values were derived. \n",
    "    @Parameter\n",
    "    ----------\n",
    "    l : List of floats\n",
    "        List of valence or arousal values\n",
    "    length : Int\n",
    "        Length of original list of words\n",
    "    \"\"\"\n",
    "    if len(l) > 0:\n",
    "        if length is not None:\n",
    "            return (sum(l) + 0.5*(length-len(l))) / length\n",
    "        else:\n",
    "            return sum(l)/len(l)\n",
    "    else:\n",
    "        return 0.5\n",
    "\n",
    "def filter_toks(toks, lexicon):\n",
    "    \"\"\"\n",
    "    A function that combines single word tokens into bigrams when\n",
    "    said bigrams exist in lexicon and filters out words that do not\n",
    "    exist in lexicon\n",
    "    @Parameter\n",
    "    ----------\n",
    "    toks : List of str\n",
    "        Tokens to compare to lexicon\n",
    "    lexicon : List of str\n",
    "        Lexicon of words of interest\n",
    "    @Return\n",
    "    -------\n",
    "    filtered_toks : list of str\n",
    "        Tokens that exist in lexicon, prioritizing bigrams over tokens\n",
    "    \"\"\"\n",
    "    filtered_toks = list()\n",
    "    finishing_bigram = False\n",
    "    for i in range(len(toks)):\n",
    "        if finishing_bigram:\n",
    "            finishing_bigram = False\n",
    "            continue\n",
    "        tok = toks[i]\n",
    "        tok_lemma = lemmatizer.lemmatize(tok)\n",
    "        if i == (len(toks) - 1):\n",
    "            if tok in lexicon: filtered_toks.append(tok)\n",
    "            elif tok_lemma in lexicon: filtered_toks.append(tok_lemma)\n",
    "        else:\n",
    "            bigram = toks[i] + ' ' + toks[i+1]\n",
    "            if bigram in lexicon:\n",
    "                filtered_toks.append(bigram)\n",
    "                finishing_bigram = True\n",
    "            else:\n",
    "                bigram = toks[i] + ' ' + lemmatizer.lemmatize(toks[i+1])\n",
    "                if bigram in lexicon:\n",
    "                    filtered_toks.append(bigram)\n",
    "                    finishing_bigram = True\n",
    "                elif tok in lexicon:\n",
    "                    filtered_toks.append(tok)\n",
    "                elif tok_lemma in lexicon:\n",
    "                    filtered_toks.append(tok_lemma)\n",
    "    return filtered_toks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "valence_lexicon = pd.read_csv(os.path.join(os.getcwd(), \"helper_data/nrc_vad_lex/v-scores.txt\"),\n",
    "                              names=['word', 'valence'], sep='\\t', keep_default_na=False, na_values=['nan', 'NaN'])\n",
    "\n",
    "arousal_lexicon = pd.read_csv(os.path.join(os.getcwd(), \"helper_data/nrc_vad_lex/a-scores.txt\"),\n",
    "                              names=['word', 'arousal'], sep=\"\\t\", keep_default_na=False, na_values=['nan', 'NaN'])\n",
    "valence_lexicon['word'] = valence_lexicon['word'].astype(str)\n",
    "arousal_lexicon['word'] = arousal_lexicon['word'].astype(str)\n",
    "vad_lexicon = valence_lexicon.merge(arousal_lexicon, on='word').set_index('word')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "vad_words = vad_lexicon.index.to_list()\n",
    "survey_text_df['pros_arousal'] = survey_text_df['pros_toks'].apply(lambda toks :\n",
    "    list_mean([vad_lexicon.loc[tok, 'arousal'] for tok in filter_toks(toks, vad_words)]))\n",
    "survey_text_df['pros_valence'] = survey_text_df['pros_toks'].apply(lambda toks :\n",
    "    list_mean([vad_lexicon.loc[tok, 'valence'] for tok in filter_toks(toks, vad_words)]))\n",
    "\n",
    "survey_text_df['cons_arousal'] = survey_text_df['cons_toks'].apply(lambda toks :\n",
    "    list_mean([vad_lexicon.loc[tok, 'arousal'] for tok in filter_toks(toks, vad_words)]))\n",
    "survey_text_df['cons_valence'] = survey_text_df['cons_toks'].apply(lambda toks :\n",
    "    list_mean([vad_lexicon.loc[tok, 'valence'] for tok in filter_toks(toks, vad_words)]))\n",
    "\n",
    "survey_text_df['glassdoor_arousal'] = survey_text_df['glassdoor_toks'].apply(lambda toks :\n",
    "    list_mean([vad_lexicon.loc[tok, 'arousal'] for tok in filter_toks(toks, vad_words)]))\n",
    "survey_text_df['glassdoor_valence'] = survey_text_df['glassdoor_toks'].apply(lambda toks :\n",
    "    list_mean([vad_lexicon.loc[tok, 'valence'] for tok in filter_toks(toks, vad_words)]))\n",
    "\n",
    "# the weighted version considrs words not included in lexicon to have 0.5 valence and 0.5 arousal\n",
    "survey_text_df['pros_arousal_weighted'] = survey_text_df['pros_toks'].apply(lambda toks :\n",
    "    list_mean([vad_lexicon.loc[tok, 'arousal'] for tok in filter_toks(toks, vad_words)], length=len(toks)))\n",
    "survey_text_df['pros_valence_weighted'] = survey_text_df['pros_toks'].apply(lambda toks :\n",
    "    list_mean([vad_lexicon.loc[tok, 'valence'] for tok in filter_toks(toks, vad_words)], length=len(toks)))\n",
    "\n",
    "survey_text_df['cons_arousal_weighted'] = survey_text_df['cons_toks'].apply(lambda toks :\n",
    "    list_mean([vad_lexicon.loc[tok, 'arousal'] for tok in filter_toks(toks, vad_words)], length=len(toks)))\n",
    "survey_text_df['cons_valence_weighted'] = survey_text_df['cons_toks'].apply(lambda toks :\n",
    "    list_mean([vad_lexicon.loc[tok, 'valence'] for tok in filter_toks(toks, vad_words)], length=len(toks)))\n",
    "\n",
    "survey_text_df['glassdoor_arousal_weighted'] = survey_text_df['glassdoor_toks'].apply(lambda toks :\n",
    "    list_mean([vad_lexicon.loc[tok, 'arousal'] for tok in filter_toks(toks, vad_words)], length=len(toks)))\n",
    "survey_text_df['glassdoor_valence_weighted'] = survey_text_df['glassdoor_toks'].apply(lambda toks :\n",
    "    list_mean([vad_lexicon.loc[tok, 'valence'] for tok in filter_toks(toks, vad_words)], length=len(toks)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Should average arousal be \n",
    "1) average arousal of words with arousal values. Problem: short responses with a couple generic arousal words are rated very highly\n",
    "2) average arousal of the entire list, such that words not in the VAD are essentially assigned a value of 0 arousal. average arousal of words, weighted by the proportion of words with arousal values. Problem: assuming that words that are not present lack arousal\n",
    "3) average arousal of the entire list, such that words not in the VAD are assigned a value of 0.5, the average arousal and valence value\n",
    "Given that we are controlling for tok length and only comparing responses with the same length, this shouldn't be much of a problem\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.614\n",
      "0.5466363636363636\n",
      "0.2511818181818182\n"
     ]
    }
   ],
   "source": [
    "from statistics import mean\n",
    "word_val = [(tok, vad_lexicon.loc[tok]['arousal']) if tok in vad_words else (tok, ) for tok in\n",
    "            \"its a great place to work with fun joy happiness i see no such downsides its a pleasure to work at collabera\".split(' ')]\n",
    "print(mean([pair[1] for pair in word_val if len(pair) > 1 ]))\n",
    "print(mean([pair[1]  if len(pair) > 1  else 0.5 for pair in word_val]))\n",
    "print(mean([pair[1] for pair in word_val if len(pair) > 1 ])\n",
    "      * (len([1 for pair in word_val if len(pair) > 1]) / len(word_val)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pronouns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizerFast.from_pretrained('bert-base-uncased')\n",
    "model = DistilBertForMaskedLM.from_pretrained('distilbert-base-uncased', return_dict=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "def get_token_type(text):\n",
    "    doc = nlp(text)\n",
    "    index2type = {}\n",
    "    for token in doc:\n",
    "        index2type[(token.idx, token.idx+len(token.text))] = token.dep_\n",
    "    return index2type\n",
    "\n",
    "def we_prob(text, tokenizer, model, type2pronouns, filter_we=True, we_index=-2):\n",
    "    \"\"\"\n",
    "    Returns a tuple containing the probability of we-words and probability of we-words\n",
    "    as weighted by the total probability of relevant pronouns, where relevant pronouns\n",
    "    refer to pronouns of the same type, i.e., subject to subject, possessive to possessive.\n",
    "    The reason that type matching is important is due to the model's ability to \n",
    "    infer correct type based on context, which needs to be accounted for when weighting probabilities.\n",
    "    \"\"\"\n",
    "    text = text.lower()\n",
    "    encoded_dict = tokenizer(text, return_offsets_mapping=True)\n",
    "    toks = tokenizer.convert_ids_to_tokens(encoded_dict['input_ids'])\n",
    "    pronoun2type = {pronoun : k for k, v in type2pronouns.items() for pronoun in v}\n",
    "    pronouns = [pronoun for k, v in type2pronouns.items() for pronoun in v]\n",
    "\n",
    "    found_we_word = True if len(set(toks) & set(we_words)) > 0 else False\n",
    "    found_pronouns = True if len(set(toks) & set(pronouns)) > 0 else False\n",
    "    if (filter_we and not found_we_word) or not found_pronouns:\n",
    "        return np.nan\n",
    "    index2type = get_token_type(text)\n",
    "    # with pytorch format, the results can't be converted to tokens directly\n",
    "    # if direct conversation needed, remove return_tensors argument\n",
    "    input_ids = tokenizer(text, return_tensors=\"pt\")[\"input_ids\"]\n",
    "    # chopping input_ids to deal with cases where there are more than 512 tokens\n",
    "    # there is currently only one instance with cons where more than 512 tokens exist\n",
    "    # if this method is actually used to build model for glassdoor, should explore\n",
    "    # other ways to deal with this problem - e.g., sliding window\n",
    "    if input_ids.shape[1] > 512: \n",
    "        input_ids = input_ids[:, 0:511]\n",
    "        toks = toks[0:511]\n",
    "    masked_index = []\n",
    "    skip = False\n",
    "    # mask all pronouns to prevent leaking.\n",
    "    for i in range(input_ids.shape[1]):\n",
    "        if skip:\n",
    "            skip = False\n",
    "            continue\n",
    "        curr_id = input_ids[0, i]\n",
    "        curr_tok = tokenizer.convert_ids_to_tokens(curr_id.item())\n",
    "        if curr_tok in pronouns:\n",
    "            input_ids[0, i] = tokenizer.mask_token_id\n",
    "            # the following word has to be a verb, which needs to be\n",
    "            # masked due to verb tenses leaking the right pronoun\n",
    "            if pronoun2type[curr_tok] == 'nsubj':\n",
    "                input_ids[0, i+1] = tokenizer.mask_token_id\n",
    "                skip = True             \n",
    "            if not filter_we:\n",
    "                masked_index.append(i)\n",
    "            elif curr_tok in we_words:\n",
    "                masked_index.append(i)\n",
    "    outputs = model(input_ids)\n",
    "    prediction_logits = outputs.logits\n",
    "    raw_prob, prop_prob = [], []\n",
    "    for m in masked_index:\n",
    "        orig_word = toks[m]           \n",
    "        target_words = type2pronouns[pronoun2type[orig_word]]\n",
    "        # \"you\" could be a subject or an object\n",
    "        if orig_word == 'you':\n",
    "            if encoded_dict['offset_mapping'][m] not in index2type.keys():\n",
    "                raise ValueError('Token has no matching type. See text %s:' % text)\n",
    "            else:\n",
    "                # obj, pobj, dobj, dative - indirect object\n",
    "                if 'obj' in index2type[encoded_dict['offset_mapping'][m]] or 'dative' == index2type[encoded_dict['offset_mapping'][m]]:\n",
    "                    target_words = type2pronouns['obj']\n",
    "                elif 'subj' in index2type[encoded_dict['offset_mapping'][m]]:\n",
    "                    target_words = type2pronouns['subj']\n",
    "                else:\n",
    "                    raise ValueError('Unexpected dependency type: %s' % index2type[encoded_dict['offset_mapping'][m]])\n",
    "        we_word = target_words[we_index]\n",
    "        logits = prediction_logits[0, m, :]\n",
    "        # the original code for single mask use case uses dim=0 as there is only one dim\n",
    "        # which also works here given that we are iterating across masks one at a time\n",
    "        probs = logits.softmax(dim=-1)\n",
    "        target_inds = np.array(tokenizer.convert_tokens_to_ids(target_words))\n",
    "        # of shape number of we-word masks by number of target words\n",
    "        values = probs[..., target_inds]\n",
    "        we_word_prob = probs[..., tokenizer.convert_tokens_to_ids(we_word)].item()\n",
    "        we_word_prop = we_word_prob / values.sum().item()\n",
    "        raw_prob.append(we_word_prob)\n",
    "        prop_prob.append(we_word_prop)\n",
    "    return (sum(raw_prob)/len(raw_prob), sum(prop_prob)/len(prop_prob))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next Steps\n",
    "1) We are currently restricting to we-words to calculate the probability of we over all words and probability of we over all pronouns. Given that high probability of we seems to be positively related to identification, we could also try masking all pronouns and looking at probability of we over all words and pronouns\n",
    "2) We should try the original measure but include it  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Original Measure\n",
    "type2pronouns = {'subj': ['i', 'you', 'he', 'she', 'we', 'they'],\n",
    "           'obj': ['me', 'you', 'him', 'her', 'us', 'them'],\n",
    "           'poss': ['my', 'your', 'his', 'her', 'our', 'their'],\n",
    "           'attr': ['mine', 'yours', 'his', 'hers', 'ours', 'theirs'],\n",
    "           'npadvmod': ['myself', 'yourself', 'yourselves', 'himself', 'herself', 'ourselves', 'themselves']}\n",
    "\n",
    "def average_with_nan(elem1, elem2):\n",
    "    if np.isnan(elem1) and np.isnan(elem2):\n",
    "        return np.nan\n",
    "    elif np.isnan(elem1):\n",
    "        return elem2\n",
    "    elif np.isnan(elem2):\n",
    "        return elem1\n",
    "    else:\n",
    "        return (elem1+elem2)/2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "we_prob_pros = survey_text_df['pros_cleaned'].apply(lambda text : we_prob(text, tokenizer, model, type2pronouns, filter_we=True))\n",
    "we_prob_cons = survey_text_df['cons_cleaned'].apply(lambda text : we_prob(text, tokenizer, model, type2pronouns, filter_we=True))\n",
    "we_prob_pros_df = pd.DataFrame(we_prob_pros.tolist(), columns=['pros_we_prob', 'pros_we_prop'])\n",
    "we_prob_cons_df = pd.DataFrame(we_prob_cons.tolist(), columns=['cons_we_prob', 'cons_we_prop'])\n",
    "# Drop if columns already exist\n",
    "survey_text_df = survey_text_df.drop(['pros_we_prob', 'pros_we_prop', 'cons_we_prob', 'cons_we_prop'], axis=1, errors='ignore')\n",
    "survey_text_df = pd.concat([survey_text_df, we_prob_pros_df, we_prob_cons_df], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uses All Pronouns (except for it - too many typos)\n",
    "we_prob_pros = survey_text_df['pros_cleaned'].apply(lambda text : we_prob(text, tokenizer, model, type2pronouns, filter_we=False))\n",
    "we_prob_cons = survey_text_df['cons_cleaned'].apply(lambda text : we_prob(text, tokenizer, model, type2pronouns, filter_we=False))\n",
    "we_prob_pros_df = pd.DataFrame(we_prob_pros.tolist(), columns=['pros_all_prob', 'pros_all_prop'])\n",
    "we_prob_cons_df = pd.DataFrame(we_prob_cons.tolist(), columns=['cons_all_prob', 'cons_all_prop'])\n",
    "survey_text_df = pd.concat([survey_text_df, we_prob_pros_df, we_prob_cons_df], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "survey_text_df['glassdoor_we_prob'] = survey_text_df.apply(lambda row : average_with_nan(row['pros_we_prob'], row['cons_we_prob']), axis=1)\n",
    "survey_text_df['glassdoor_we_prop'] = survey_text_df.apply(lambda row : average_with_nan(row['pros_we_prop'], row['cons_we_prop']), axis=1)\n",
    "survey_text_df['glassdoor_all_prob'] = survey_text_df.apply(lambda row : average_with_nan(row['pros_all_prob'], row['cons_all_prob']), axis=1)\n",
    "survey_text_df['glassdoor_all_prop'] = survey_text_df.apply(lambda row : average_with_nan(row['pros_all_prop'], row['cons_all_prop']), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uses All Pronouns (except for it - too many typos)\n",
    "we_prob_pros = survey_text_df['pros_cleaned'].apply(lambda text : we_prob(text + ' i work at collabera.', tokenizer, model, type2pronouns, filter_we=False))\n",
    "we_prob_cons = survey_text_df['cons_cleaned'].apply(lambda text : we_prob(text + ' i work at collabera.', tokenizer, model, type2pronouns, filter_we=False))\n",
    "we_prob_pros_df = pd.DataFrame(we_prob_pros.tolist(), columns=['pros_inflated_prob', 'pros_inflated_prop'])\n",
    "we_prob_cons_df = pd.DataFrame(we_prob_cons.tolist(), columns=['cons_inflated_prob', 'cons_inflated_prop'])\n",
    "survey_text_df = pd.concat([survey_text_df, we_prob_pros_df, we_prob_cons_df], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finetuning on Survey Responses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The chunks below have been converted to Raw NBConvert as the finetuned distilbert model has already been saved to disk and does not need to be retrained everytime. If retraining needed, simply convert back to Code."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
    "import torch.utils.data as data_utils\n",
    "from transformers import LineByLineTextDataset\n",
    "import csv\n",
    "filepath = '/Users/Lara/Documents/CompCulture/spacespace/Coco/helper_data/corpus.txt'\n",
    "np.savetxt(filepath, survey_text_df['glassdoor_cleaned'], fmt=\"%s\")\n",
    "\n",
    "tokenizer = BertTokenizerFast.from_pretrained('bert-base-uncased')\n",
    "model = DistilBertForMaskedLM.from_pretrained('distilbert-base-uncased')\n",
    "\n",
    "dataset = LineByLineTextDataset(\n",
    "    tokenizer=tokenizer,\n",
    "    file_path=filepath,\n",
    "    block_size=128,\n",
    ")"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "from transformers import Trainer, TrainingArguments, DataCollatorForLanguageModeling\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',          # output directory\n",
    "    num_train_epochs=1,              # total # of training epochs\n",
    "    per_device_train_batch_size=16,  # batch size per device during training\n",
    "    per_device_eval_batch_size=64,   # batch size for evaluation\n",
    "    warmup_steps=500,                # number of warmup steps for learning rate scheduler\n",
    "    weight_decay=0.01,               # strength of weight decay\n",
    "    logging_dir='./logs',            # directory for storing logs\n",
    ")\n",
    "\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer, mlm=True, mlm_probability=0.15\n",
    ")\n",
    "trainer = Trainer(\n",
    "    model=model,                         # the instantiated ü§ó Transformers model to be trained\n",
    "    args=training_args,                  # training arguments, defined above\n",
    "    train_dataset=dataset,         # training dataset\n",
    "    data_collator=data_collator,\n",
    ")"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "%%time\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "trainer.save_model(\"./finetuned_distilbert\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyzing Responses Using Finetuned Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = DistilBertForMaskedLM.from_pretrained(\"./finetuned_distilbert\", return_dict=True)\n",
    "# Uses All Pronouns (except for it - too many typos)\n",
    "we_prob_pros = survey_text_df['pros_cleaned'].apply(lambda text : we_prob(text, tokenizer, model, type2pronouns, filter_we=False))\n",
    "we_prob_cons = survey_text_df['cons_cleaned'].apply(lambda text : we_prob(text, tokenizer, model, type2pronouns, filter_we=False))\n",
    "we_prob_pros_df = pd.DataFrame(we_prob_pros.tolist(), columns=['pros_finetuned_prob', 'pros_finetuned_prop'])\n",
    "we_prob_cons_df = pd.DataFrame(we_prob_cons.tolist(), columns=['cons_finetuned_prob', 'cons_finetuned_prop'])\n",
    "survey_text_df = pd.concat([survey_text_df, we_prob_pros_df, we_prob_cons_df], axis=1)\n",
    "\n",
    "survey_text_df['glassdoor_finetuned_prob'] = survey_text_df.apply(lambda row : average_with_nan(row['pros_finetuned_prob'], row['cons_finetuned_prob']), axis=1)\n",
    "survey_text_df['glassdoor_finetuned_prop'] = survey_text_df.apply(lambda row : average_with_nan(row['pros_finetuned_prop'], row['cons_finetuned_prop']), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "survey_text_df.drop(columns=['index']).set_index('uid').sort_index().to_csv('~/Documents/CompCulture/spacespace/Coco/analyses_data/survey_hr_deductive_vars.csv', index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example Code for Calculating Word-Level Probabilities Using Contextual Embeddings\n",
    "To run the cell below, simply convert it back to code"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "text = \"The best part working with Collabera is, its an employee centric company. [MASK] [MASK] employees irrespective of level/designation and give equal opportunity to everyone. \"\n",
    "input_ids = tokenizer(text, return_tensors=\"pt\")[\"input_ids\"]\n",
    "outputs = model(input_ids)\n",
    "prediction_logits = outputs.logits\n",
    "target_word = ['i', 'you', 'he', 'she', 'we', 'they']\n",
    "\n",
    "# Both of these sections of code accomplish the same objective\n",
    "# the first section is written based on example code on how to use DistilBertForMaskedLM\n",
    "# the second section is adapted from source code of the fill-mask task in the nlp pipeline\n",
    "# When only one word is masked, the same objective can be accomplished by\n",
    "# unmasker = pipeline('fill-mask', model='distilbert-base-uncased')\n",
    "# unmasker(text, targets=target_word)\n",
    "# The puprpose of the code here is to allow masking for multiple words as\n",
    "# multiple words should be masked to ensure the right pronoun isn't leaked\n",
    "\n",
    "# First solution for multiple masks:\n",
    "# This solution only allows us to compute probabilities for the first instance of the MASK\n",
    "key_index = input_ids.tolist()[0].index(tokenizer.mask_token_id)\n",
    "raw_logit = prediction_logits[0, key_index, tokenizer.convert_tokens_to_ids(target_word)]\n",
    "raw_prob = prediction_logits[0, key_index, :].softmax(dim=0)[tokenizer.convert_tokens_to_ids(target_word)]\n",
    "\n",
    "# Second solution for multiple masks:\n",
    "# This solution allows for computing probabilities for multiple masked words at the same time\n",
    "# The function we_prob is based on the second solution\n",
    "# returns a tensor of size z by n, where z is the number of masked elements\n",
    "# and n is the dimension of the input tensor, which is 1D below\n",
    "masked_index = torch.nonzero(input_ids[0] == tokenizer.mask_token_id, as_tuple=False)\n",
    "# the original code for single mask use case uses masked_index.item()\n",
    "# squeeze is used here to get rid of the last dimension of masked_index\n",
    "logits = prediction_logits[0, masked_index.squeeze(), :]\n",
    "# the original code for single mask use case uses dim=0 as there is only one dim\n",
    "probs = logits.softmax(dim=-1)\n",
    "target_inds = np.array(tokenizer.convert_tokens_to_ids(target_word))\n",
    "values = probs[..., target_inds[0]]\n",
    "# if there are multiple arguments, use this to sort\n",
    "sort_inds = list(reversed(values.argsort(dim=-1)))\n",
    "values = values[..., sort_inds]\n",
    "predictions = target_inds[sort_inds]\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
