{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sys\n",
    "import os\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('max_columns', None) # copy this line of code into the cell where we want to display all columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explore Survey Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = \"~/Documents/CompCulture/Collabera/Data\"\n",
    "fn = os.path.join(data_dir, \"Collabera_Survey_Responses_all.csv\")\n",
    "survey_df = pd.read_csv(fn, index_col=['ExternalReference'], header=0,\n",
    "                        usecols=['ExternalReference', 'Progress', 'Duration (in seconds)', 'Finished', 'ResponseId',\n",
    "               'LocationLatitude', 'LocationLongitude', 'Q35_1', 'Q35_2', 'Q35_3', 'Q35_4', 'Q35_5', 'Q35_6',\n",
    "               'Q40', 'Q41', 'Q34_1', 'Q34_2', 'Q34_3', 'Q34_4', 'Q34_5', 'Q34_6',\n",
    "               'Q36', 'Q37', 'Q38'])\n",
    "survey_df = survey_df[2:]\n",
    "survey_df = survey_df.rename(columns={\"Q35_1\":\"mael_1\", \"Q35_2\":\"mael_2\", \"Q35_3\":\"mael_3\",\n",
    "                                      \"Q35_4\":\"mael_4\", \"Q35_5\":\"mael_5\", \"Q35_6\":\"mael_6\",\n",
    "                                      \"Q40\":\"bergami_org\", \"Q41\":\"bergami_dept\",\n",
    "                                      \"Q34_1\":\"disengagement_1\", \"Q34_2\":\"exhaustion_1\", \"Q34_3\":\"exhaustion_2\",\n",
    "                                      \"Q34_4\":\"exhaustion_3\", \"Q34_5\":\"disengagement_2\", \"Q34_6\":\"disengagement_3\",\n",
    "                                      \"Q36\":\"pros\", \"Q37\":\"cons\", \"Q38\":\"story\"})\n",
    "survey_df = survey_df.astype({'Progress':'int32'})\n",
    "survey_df = survey_df.loc[survey_df.index.notna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def numerify(df, col_name, new_col=None, likert=5):\n",
    "    if not new_col:\n",
    "        new_col = col_name\n",
    "    if likert == 5:\n",
    "        df.loc[df[col_name] == 'Strongly agree', new_col] = 5\n",
    "        df.loc[df[col_name] == 'Somewhat agree', new_col] = 4\n",
    "        df.loc[df[col_name] == 'Neither agree nor disagree', new_col] = 3\n",
    "        df.loc[df[col_name] == 'Somewhat disagree', new_col] = 2\n",
    "        df.loc[df[col_name] == 'Strongly disagree', new_col] = 1\n",
    "    elif likert == 4:\n",
    "        df.loc[df[col_name] == 'Strongly agree', new_col] = 4\n",
    "        df.loc[df[col_name] == 'Agree', new_col] = 3\n",
    "        df.loc[df[col_name] == 'Disagree', new_col] = 2\n",
    "        df.loc[df[col_name] == 'Strongly disagree', new_col] = 1\n",
    "    return df\n",
    "\n",
    "def letter_to_number(df, col_name, new_col=None):\n",
    "    if not new_col:\n",
    "        new_col = col_name\n",
    "    letters = ['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H']\n",
    "    for i in range(8):\n",
    "        df.loc[df[col_name] == letters[i], new_col] = i + 1\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "for s in range(1, 7):\n",
    "    survey_df = numerify(survey_df, 'mael_'+str(s), likert=5)\n",
    "for s in range(1, 4):\n",
    "    survey_df = numerify(survey_df, 'disengagement_'+str(s), likert=4)\n",
    "    survey_df = numerify(survey_df, 'exhaustion_'+str(s), likert=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "survey_df = letter_to_number(survey_df, 'bergami_org', 'bergami_org_num')\n",
    "survey_df = letter_to_number(survey_df, 'bergami_dept', 'bergami_dept_num')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "survey_df['mael_avg'] = survey_df.apply(lambda row : (row['mael_1'] + row['mael_2'] + row['mael_3'] + row['mael_4'] + row['mael_5'] + row['mael_6'])/6,\n",
    "                                        axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explore HR Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "hr = os.path.join(data_dir, \"Collabera_HR_Perf.csv\")\n",
    "hr_df = pd.read_csv(hr, index_col=['UID'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'Achieved': 846, nan: 733, 'Not Achieved': 146, 'Not Applicable': 2})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set(['U'+str(i) for i in range(1, 1751)]) - (set(hr_df.index.to_list()))\n",
    "# Figuring out why six observations are missing if we drop NA (the number of rows change from 1727 to 1721 if we run hr_df.drop_na())\n",
    "temp_hr = hr_df.drop(['2019 Performance', '2020 Performance'], axis=1)\n",
    "temp_hr[temp_hr.isnull().any(axis=1)] # EEO Code missing for six observations\n",
    "from collections import Counter\n",
    "# used the line below to repeatedly investigate unique values of each column\n",
    "sorted(Counter(hr_df['Department'].to_list()).items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "Counter(hr_df['2020 Performance'].to_list())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fixing HR Data Issues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Expand state abbreviations discovered while exploring each column in the cell above\n",
    "hr_df.loc[hr_df['Work State'] == 'CO', 'Work State'] = 'Colorado'\n",
    "hr_df.loc[hr_df['Work State'] == 'WI', 'Work State'] = 'Wisconsin'\n",
    "hr_df.loc[hr_df['Work State'] == 'NC', 'Work State'] = 'North Carolina'\n",
    "# Create extra dummy for Collabera emps, excluding those working for Cognixia and Webxl\n",
    "hr_df['Collabera'] = hr_df['Legal Entity Name'].apply(lambda s : 1 if 'Collabera' in s else 0)\n",
    "# Fix race data: change nan entries to missing, collapse all Hispanic or Latino into Hispanic or Latino \n",
    "hr_df['Race'] = hr_df['EEO Code']\n",
    "hr_df.loc[hr_df['Race'].isnull(), 'Race'] = 'Missing'\n",
    "hr_df.loc[hr_df['Race'].str.contains('Hispanic or Latino'), 'Race'] = 'Hispanic or Latino'\n",
    "hr_df.loc[hr_df['Race'] == 'Race missing or unknown', 'Race'] = 'Missing'\n",
    "# Division, department, and function all look fine - no typos or mistakes\n",
    "\n",
    "# Changing rows with Not Applicable into NAN to allow for future dropping of NAN performance entries\n",
    "hr_df.loc[hr_df['2019 Performance'].isnull(), '2019 Performance'] = 'Not Applicable'\n",
    "hr_df.loc[hr_df['2020 Performance'].isnull(), '2020 Performance'] = 'Not Applicable'\n",
    "\n",
    "hr_df['2019_perf_dummy'] = np.nan\n",
    "hr_df['2020_perf_dummy'] = np.nan\n",
    "hr_df.loc[hr_df['2019 Performance'] == 'Not Achieved', '2019_perf_dummy'] = 0\n",
    "hr_df.loc[hr_df['2020 Performance'] == 'Not Achieved', '2020_perf_dummy'] = 0\n",
    "\n",
    "hr_df.loc[hr_df['2019 Performance'] == 'Achieved', '2019_perf_dummy'] = 1\n",
    "hr_df.loc[hr_df['2020 Performance'] == 'Achieved', '2020_perf_dummy'] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "survey_hr_df = survey_df.join(hr_df, how='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(713, 45)\n",
      "(852, 45)\n"
     ]
    }
   ],
   "source": [
    "print(survey_hr_df.dropna(subset=['2020_perf_dummy']).loc[survey_hr_df['Progress'] == 100,].shape)\n",
    "print(survey_hr_df.dropna(subset=['2020_perf_dummy']).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "survey_hr_df.to_csv('~/Documents/CompCulture/spacespace/Coco/analyses_data/preprocessed_survey_hr.csv', index_label='uid')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploring and Coding Survey Responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_colwidth', 0)\n",
    "answers = []\n",
    "num_responses = 50\n",
    "survey_text_df = survey_hr_df.dropna(subset=['pros', 'cons', 'story']).astype({'pros':'str',\n",
    "                             'cons':'str',\n",
    "                             'story':'str'})\n",
    "sample_df = survey_text_df[['Gender', 'Race', 'mael_avg', 'bergami_org', 'bergami_dept', 'pros','cons','story']].sample(num_responses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i in range(num_responses):\n",
    "    display(sample_df.iloc[[i]])\n",
    "    inp = input(\"Notes:\\t\")\n",
    "    answers.append(inp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_df['Coding Notes'] = answers\n",
    "sample_df.to_csv(\"~/Documents/CompCulture/spacespace/Coco/analyses_data/sampled_responses_notes_09302020.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "pd.set_option('display.max_colwidth', 0)\n",
    "survey_text_df.reindex(survey_text_df['bergami_org'].sort_values(ascending=False).index)[['Gender', 'Race','mael_avg', 'bergami_org', 'pros','cons','story']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quick thoughts from random coding of 50 responses:\n",
    "Potentially identify topics, such as growth, family, that predict identification? Not finding linguistic signature of identification but what sort of topics predict identification?\n",
    "\n",
    "Quick thoughts from coding top 30 and bottom 30 in bergami org identification:\n",
    "Mark individuals who have repeated responses\n",
    "A lot of references to family\n",
    "Lack of personal life - contributes to identification?\n",
    "Memories\n",
    "ownership: Collabera has given me the best platform to grow and analyze my skills to the top level. Also it has helped me become a better person. For me Collabera is the my very own company and I see ways and out how to make myself most useful in the growth of the company. Collabera's culture and environment has given me the vibes, that I am a part of it and has recognized me to the top level.\n",
    "is similarity betweeen survey response and official company language a measure of identification? Some people have really absorbed the 'we are the best' sentiment\n",
    "doesn't seem to be enough signal in survey responses especially when they are short.  Some poeple make it clear that they identify: e.g., \"We at Collabera follow our culture aggressively. Work Hard Play Hard and Insanely competitive are the best which defines us to the core. Nothing is negative here - People are positive so do our Company :)\". But a lack of we- language doesn't necessarily mean low identification\n",
    "Similarity between response and average response predicts identification? the idea that the more you embody the language of everyone, the more you identify?\n",
    "Growth seems to be a pro for people who identify and dont identify\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Micellaneous"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some of these analyses I ran here using log_we_they and log_we_i are marginally significant (log_we_they/mael and log_we_they/bergami_org) if using survey_df instead of survey_hr_df. These columns have been removed so that downstream consequences can use them more freely. The results also depend on which rows are dropped and which rows are kept - dropping rows only missing in text data or dropping rows missing in any data. As these correlations are preliminary and not accounting for any control variables, the analyses should be examined more carefully in R later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Great example\n",
    "pd.set_option('display.max_colwidth', 0)\n",
    "\n",
    "survey_text_df.loc['U169', ['pros', 'cons', 'story']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
